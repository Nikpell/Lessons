{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5T7RkBNDPzR"
   },
   "outputs": [],
   "source": [
    "# pip install pyspark\n",
    "!pip install pyspark >> None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jkbxjgk5D4cc"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiXrs2yZH9GV"
   },
   "source": [
    "HW_1.Spark Apache\n",
    "SQL & BigData\n",
    "\n",
    "Найти самую длинную последовательность упорядоченных чисел в RDD и вывести её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh6w9D4QHy4m",
    "outputId": "d838fdce-17f7-46fd-e838-d6d8191fa5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самая длинная последовательность: [10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def find_long_sequence(rdd):\n",
    "    # Преобразуем элементы RDD в целые числа и отсортируем их\n",
    "    ordered_rdd = rdd.map(lambda x: int(x)).sortBy(lambda x: x).collect()\n",
    "\n",
    "    # Создадим переменные для текущей последовательности и самой длинной последовательности\n",
    "    current_sequence = []\n",
    "    long_sequence = []\n",
    "\n",
    "    for num in ordered_rdd:\n",
    "        if not current_sequence or num == current_sequence[-1] + 1:\n",
    "            current_sequence.append(num)\n",
    "        else:\n",
    "            if len(current_sequence) > len(long_sequence):\n",
    "                long_sequence = current_sequence.copy()\n",
    "            current_sequence = [num]\n",
    "\n",
    "    # Сравниваем длину текущей последовательности с длиной самой длинной последовательности\n",
    "    if len(current_sequence) > len(long_sequence):\n",
    "        long_sequence = current_sequence\n",
    "\n",
    "    return long_sequence\n",
    "\n",
    "# Создание Spark-сессии\n",
    "spark = SparkSession.builder.appName(\"longest_sequence\").getOrCreate()\n",
    "\n",
    "# Пример входного RDD с последовательностью чисел\n",
    "data = [1, 2, 3, 5, 6, 7, 10, 11, 12, 13]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Находим самую длинную последовательность упорядоченных чисел в RDD\n",
    "result = find_long_sequence(rdd)\n",
    "\n",
    "print(\"Самая длинная последовательность:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_84TDDOrMLhS"
   },
   "outputs": [],
   "source": [
    "# Завершение Spark-сессии\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "randvalues = [random.randint(0, 120) for _ in range(55)]\n",
    "sc = SparkContext(\"local\", \"Longest_sequence\")\n",
    "rdd = sc.parallelize(randvalues)\n",
    "longest_sequence = rdd.map(lambda x: (x, 1)) \\\n",
    "        .reduceByKey(lambda x, y: x + y).max(key=lambda x: x[1])\n",
    "\n",
    "print(randvalues)\n",
    "print(f'{\"Самая длинная последовательность упорядоченных чисел:\":^50}')\n",
    "\n",
    "print(list(range(longest_sequence[0] - longest_sequence[1] + 1, longest_sequence[0] + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [55, 81, 16, 88, 14, 87, 53, 12, 4, 24, 101, 39, 9, 30, 84, 68, 33, 14, 98, 65, 76, 15, 75, 30, 109, 57, 120, 0, 67, 40, 73, 78, 93, 107, 37, 75, 120, 79, 85, 92, 93, 1, 112, 52, 2, 82, 35, 105, 4, 73, 93, 84, 100, 8, 65]\n",
    "# Самая длинная последовательность упорядоченных чисел:\n",
    "# [91, 92, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.choice(100, 50, replace=False)\n",
    "print(data)\n",
    "\n",
    "sc = SparkContext(\"local\", \"Longest sequence RDD\")\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "\n",
    "def find_longest_sequence_partitions(iterator):\n",
    "    sequence = sorted(iterator)\n",
    "    result = []\n",
    "    current_sequence = []\n",
    "\n",
    "    for num in sequence:\n",
    "        if not current_sequence or num == current_sequence[-1] + 1:\n",
    "            current_sequence.append(num)\n",
    "        else:\n",
    "            if len(current_sequence) > len(result):\n",
    "                result = current_sequence\n",
    "            current_sequence = [num]\n",
    "\n",
    "    if len(current_sequence) > len(result):\n",
    "        result = current_sequence\n",
    "\n",
    "    yield result\n",
    "\n",
    "def merge_sequences(seq1, seq2):\n",
    "    merged = sorted(seq1 + seq2)\n",
    "    result = []\n",
    "    current_sequence = []\n",
    "\n",
    "    for num in merged:\n",
    "        if not current_sequence or num == current_sequence[-1] + 1:\n",
    "            current_sequence.append(num)\n",
    "        else:\n",
    "            if len(current_sequence) > len(result):\n",
    "                result = current_sequence\n",
    "            current_sequence = [num]\n",
    "\n",
    "    if len(current_sequence) > len(result):\n",
    "        result = current_sequence\n",
    "\n",
    "    return result\n",
    "\n",
    "longest_sequences = rdd.mapPartitions(find_longest_sequence_partitions).collect()\n",
    "longest_sequence = longest_sequences[0]\n",
    "for seq in longest_sequences[1:]:\n",
    "    longest_sequence = merge_sequences(longest_sequence, seq)\n",
    "\n",
    "print(\"Longest sequence:\", longest_sequence)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S17o7yLZIdwp"
   },
   "source": [
    "Создание нескольких Spark сессий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jmkb1JcaBA2U"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание основной сессии Spark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"MyApp\") \\\n",
    "  .master(\"local[2]\") \\\n",
    "  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# Создание дополнительной сессии Spark с другой конфигурацией\n",
    "\n",
    "spark2 = spark.newSession() \\\n",
    "  .config(\"spark.executor.memory\", \"1g\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii5xP9UyCMON"
   },
   "source": [
    "Литература:\n",
    "- Изучаем Spark, молниеносный анализ данных\n",
    "- Spark in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hHuVGPiFkDo",
    "outputId": "158026df-0d61-4208-fdee-4b8cc55db5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение элементов в RDD:  4.0\n"
     ]
    }
   ],
   "source": [
    "# 1/ Найти среднее значение элементов в RDD\n",
    "\n",
    "sc = SparkContext(\"local\", \"Average RDD\")\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "mean_value = rdd.mean()\n",
    "print(\"Среднее значение элементов в RDD: \", mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8HDMPHVHP2g",
    "outputId": "54636e17-4550-45fe-9824-2c043c127bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkSession.active of <class 'pyspark.sql.session.SparkSession'>>\n"
     ]
    }
   ],
   "source": [
    "active_session = SparkSession.active\n",
    "print(active_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBAObsHdH1Un"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8445JQosH7rb",
    "outputId": "6a8536a7-533b-4ac0-cebe-a85da3950f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение элементов в RDD:  55.857142857142854\n"
     ]
    }
   ],
   "source": [
    "# 2/ Найти наибольший элемент в RDD\n",
    "\n",
    "sc = SparkContext(\"local\", \"Max RDD\")\n",
    "rdd = sc.parallelize([100, 25, 30, 40, 55, 63, 78])\n",
    "max_value = rdd.mean()\n",
    "print(\"Среднее значение элементов в RDD: \", max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN5UwFHZNmJC"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gj1MRi92Nrzv",
    "outputId": "15070d68-9e29-4345-bf0c-c896326f640c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество элементов, больших 5: 5\n"
     ]
    }
   ],
   "source": [
    "# 3/ Посчитать количество элементов, удовлетворяющих условию\n",
    "\n",
    "sc = SparkContext(\"local\", \"Filter RDD\")\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "filtered_rdd = rdd.filter(lambda x: x > 5)\n",
    "count = filtered_rdd.count()\n",
    "print(\"Количество элементов, больших 5:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWteZFLxPoxW"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xknGuitLPsbh",
    "outputId": "6c100c10-f08f-42ca-8dff-c3bc0a4403f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 25), ('banana', 45), ('peach', 40)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4/ Дан набор данных с инф о продажах товаров (товар, магазин, кол).\n",
    "# Необходимо сгруппировать данные по товару и\n",
    "# найти суммарное количество проданных товаров по каждому товару\n",
    "\n",
    "sc = SparkContext(\"local\", \"advanced\")\n",
    "data = [(\"apple\", \"store1\", 10), (\"apple\", \"store2\", 15),\n",
    "        (\"banana\", \"store1\", 20), (\"banana\", \"store2\", 25),\n",
    "        (\"peach\", \"store1\", 5), (\"peach\", \"store2\", 10),\n",
    "        (\"peach\", \"store3\", 25),]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "grouped_rdd = rdd.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b: a + b)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eydB8o07T7yN"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lmz8SpQ4T9Xm",
    "outputId": "e7669a26-e52e-4657-eb82-2a4df9671954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('store1', 'apple'), 20),\n",
       " (('store2', 'apple'), 37.5),\n",
       " (('store1', 'banana'), 30.0),\n",
       " (('store2', 'banana'), 45.0),\n",
       " (('store1', 'watermelon'), 15),\n",
       " (('store2', 'watermelon'), 8)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5/ Дан набор данных с инф о продажах товаров (магазин, товар, кол, цена).\n",
    "# Необходимо найти общую выручку от продаж каждого товара в каждом магазине\n",
    "\n",
    "sc = SparkContext(\"local\", \"advanced\")\n",
    "data = [(\"store1\", \"apple\", 10, 2), (\"store2\", \"apple\", 15, 2.5),\n",
    "        (\"store1\", \"banana\", 20, 1.5), (\"store2\", \"banana\", 25, 1.8),\n",
    "        (\"store1\", \"watermelon\", 3, 5), (\"store2\", \"watermelon\", 2, 4)]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "revenue_rdd = rdd.map(lambda x: ((x[0], x[1]), x[2]*x[3])).reduceByKey(lambda a, b: a + b)\n",
    "revenue_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCRLBB2zX26A"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQCQ2CGlYFlT",
    "outputId": "d5dfc5f1-7195-4ebf-ae50-7abb397a6976"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banana', 30.0),\n",
       " ('banana', 30.0),\n",
       " ('peach', 37.5),\n",
       " ('peach', 62.5),\n",
       " ('watermelon', 17.5),\n",
       " ('watermelon', 35.0),\n",
       " ('apple', 20),\n",
       " ('apple', 30)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6/ Даны 2 набора данных: 1 - с инф о продажах (товар, кол) и 2 - с инфо о цене товара (товар, цена).\n",
    "# Необходимо объеденить данные и найти общую выручку от продаж каждого товара\n",
    "\n",
    "sc = SparkContext(\"local\", \"advanced\")\n",
    "sales_data = [(\"apple\", 10), (\"banana\", 20), (\"apple\", 15), (\"banana\", 20), (\"peach\", 15), (\"peach\", 25), (\"watermelon\", 5), (\"watermelon\", 10)]\n",
    "price_data = [(\"apple\", 2), (\"banana\", 1.5), (\"peach\", 2.5), (\"watermelon\", 3.5)]\n",
    "\n",
    "sales_rdd = sc.parallelize(sales_data)\n",
    "price_rdd = sc.parallelize(price_data)\n",
    "\n",
    "joined_rdd = sales_rdd.join(price_rdd)\n",
    "\n",
    "revenue_rdd = joined_rdd.map(lambda x: (x[0], x[1][0] * x[1][1]))\n",
    "revenue_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZOAYG3jesyx"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofxTRzcyexrS",
    "outputId": "cf5648bc-5fd1-468c-93be-e82a31c37ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое длинное слово:  путеводитель\n"
     ]
    }
   ],
   "source": [
    "# 7/ Поиск самого длинного слова в RDD, выводит первое попавшее по длине!!\n",
    "\n",
    "sc = SparkContext(\"local\", \"RDD tasks\")\n",
    "\n",
    "data = ['приложение', 'Яблоко', 'Спарк', 'путеводитель','Метрополитен','Анализ','Солнце','питон','снег','рынок']\n",
    "rdd = sc.parallelize(data)\n",
    "longest_word = rdd.max(key=lambda x: len(x))\n",
    "print(\"Самое длинное слово: \", longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2wGY4MengKR"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A85EgZFD_iz"
   },
   "outputs": [],
   "source": [
    "# 8/ Фильтрация слов по длине в RDD\n",
    "\n",
    "sc = SparkContext(\"local\", \"RDD tasks\")\n",
    "\n",
    "data = ['приложение', 'Яблоко', 'Спарк', 'путеводитель','Метрополитен','Анализ','Солнце','питон','снег','рынок']\n",
    "rdd = sc.parallelize(data)\n",
    "filtered_rdd = rdd.max(key=lambda x: len(x) > 6)\n",
    "print(\"Самое длинное слово: \", filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWKtNW5kqJj3"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ta-y5KeEGEA"
   },
   "outputs": [],
   "source": [
    "# 9/ Подсчет количества уникальных слов в RDD\n",
    "\n",
    "sc = SparkContext(\"local\", \"RDD tasks\")\n",
    "\n",
    "data = ['Приложение', 'Яблоко', 'Спарк', 'Путеводитель','Метрополитен','Анализ','Солнце','Питон',\n",
    "        'Снег', 'Рынок', 'Яблоко', 'Путеводитель','Анализ']\n",
    "rdd = sc.parallelize(data)\n",
    "unique_words_count = rdd.distinct().count()\n",
    "print(\"Количество уникальных слов:\", unique_words_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-fuRe_5cIPK"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DrvIfkOVi9n",
    "outputId": "b57c83b9-b1a5-48ef-fd25-a3ee09645fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова в верхнем регистре:  ['ПРИЛОЖЕНИЕ', 'ЯБЛОКО', 'СПАРК', 'ПУТЕВОДИТЕЛЬ', 'МЕТРОПОЛИТЕН', 'АНАЛИЗ', 'СОЛНЦЕ', 'ПИТОН', 'СНЕГ', 'РЫНОК']\n"
     ]
    }
   ],
   "source": [
    "# 10/ Преобразование всех слов в RDD в верхний регистр\n",
    "\n",
    "sc = SparkContext(\"local\", \"RDD tasks\")\n",
    "data = ['приложение', 'Яблоко', 'Спарк', 'путеводитель','Метрополитен','Анализ','Солнце','питон','снег','рынок']\n",
    "rdd = sc.parallelize(data)\n",
    "upper_rdd = rdd.map(lambda x: x.upper())\n",
    "print(\"Слова в верхнем регистре: \", upper_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RETsCKp7cmi-"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJB-18vIcocV",
    "outputId": "6f8e258e-a079-4690-afee-c4c6cbf7a304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 20.0\n",
      "6 22.0\n",
      "1 25.0\n",
      "5 28.0\n",
      "2 30.0\n"
     ]
    }
   ],
   "source": [
    "# 11/ Найти средний возраст пользователей по их покупкам и вывести топ -5 самых молодых\n",
    "\n",
    "sc = SparkContext(\"local\", \"RDD tasks\")\n",
    "user_purchase_rdd = sc.parallelize([(1, 25), (2, 30), (3, 20), (4, 35), (5, 28), (6, 22)])\n",
    "user_age_total = user_purchase_rdd.mapValues(lambda age: (age, 1)).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
    "youngest_users = user_age_total.sortBy(lambda x: x[1]).take(5)\n",
    "for user_id, avg_age in youngest_users:\n",
    "  print(user_id, avg_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z8LVRZKhL-E"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SVBRRMafdaK",
    "outputId": "db4d3aaf-8124-4f12-9b59-8460603afd71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 110.0\n",
      "B 140.0\n",
      "C 170.0\n"
     ]
    }
   ],
   "source": [
    "# 12/ Найти среднюю цену товара в каждой категории и вывести результат в формате категория-средняя цена\n",
    "sc = SparkContext(\"local\", \"ProductPraice\")\n",
    "product_rdd = sc.parallelize([(1, \"A\", 100), (2, \"B\", 150), (3, \"A\", 120), (4, \"C\", 200), (5, \"B\", 130), (6, \"C\", 140)])\n",
    "category_total_price = product_rdd.map(lambda x: (x[1], (x[2], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).mapValues(lambda v: v[0] / v[1])\n",
    "for category, avg_price in category_total_price.collect():\n",
    "  print(category, avg_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZIPZvUSjs0A"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaqnT0O2ELep"
   },
   "outputs": [],
   "source": [
    "# 13/ Найти все пары чисел из RDD, сумма которых превышает 100 и вывести их\n",
    "sc = SparkContext(\"local\", \"NumberPairs\")\n",
    "number_rdd = sc.parallelize([30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140])\n",
    "number_pairs = number_rdd.cartesian(number_rdd).filter(lambda x: x[0] + [1] > 100).collect()\n",
    "for pair in number_pairs:\n",
    "  print(pair)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
